[
  {
    "objectID": "searchable_web_table.html",
    "href": "searchable_web_table.html",
    "title": "Making a searchable database table",
    "section": "",
    "text": "#### MAKING A SEARCHABLE TABLE FOR THE WEB ####\n\n# First -- let's talk about: what is the Internet? What IS a web page?\n# An oldie but a goodie from 2009: https://www.youtube.com/watch?v=7_LPdttKXPc\n# Bottom line, it's just a bunch of computers connected to each other. \n# A website? It's just files on someone else's computer (aka server)\n\n\n#We can use the \"DT\" package to easily make a sortable, filterable, searchable data table\n#Just this little bit of code does a whole lot - check it out:\n\nDT::datatable(events)\n\n\n\n\n\n\n\n#We can already sort, but what if we want to allow the table to be FILTERED too?\n#It's easy, we just add:\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\"# <--- NEW STUFF HERE\n              )\n\n\n\n\n\n\n\n#Now hmm, what's up with the filters on the text columns? Why aren't they working?\n#It's because of a quirk in DT tables where filters will only work on text that is converted to a factor\n#So let's do that\nevents <- events %>% \n  mutate(\n    state = as_factor(state),\n    event_type = as.factor(event_type)\n  )\n\n\n#Now let's try the DT table code again and see if it worked\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\")\n\n\n\n\n\n\n\n#Now, for the coup de gr?ce\n#let's add some buttons at the top of the page to let people copy, download, etc\n#we do that using a DT \"extenstion\" called, you guessed it, Buttons\n# https://rstudio.github.io/DT/extensions.html\n\nDT::datatable(events, \n              rownames = FALSE, \n              filter = \"top\", \n              extensions = 'Buttons', \n              options = list(   # <--- NEW STUFF STARTS HERE\n                dom = 'Bfrtip',\n                buttons = c('copy', 'csv', \"excel\")\n              )) %>%\n  DT::formatStyle('cand_lastname',  color = 'red', fontWeight = 'bold')\n\n\n\n\n\n\n\n## saving the result\n\n# first we just need to assign our table to a variable...\n\nmytable <- DT::datatable(events, \n                         rownames = FALSE, \n                         filter = \"top\", \n                         extensions = 'Buttons', \n                         options = list(\n                           dom = 'Bfrtip',\n                           buttons = c('copy', 'csv', \"excel\")\n                         )) %>%\n  DT::formatStyle('cand_lastname',  color = 'red', fontWeight = 'bold') \n\n# ... then just run this simple bit of code to export to html\nDT::saveWidget(mytable, \"mytable.html\")\n\n\n# We've now created a working web page that can be put anywhere on the internet we choose\n# Yay!\n\n# If we stay within the world of quarto though we don't need to export it, we can just display it\n# within the quarto page of course\n\nWhat if we have a little table and want a super minimal table with everything stripped out\n\nevents %>% \n  select(-description) %>% \n  head(5) %>% \n  DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Marisol’s Website",
    "section": "",
    "text": "Hello! I’m Marisol Cabrera, a journalism and mass communication student at the George Washington University, based in DC and Chicago."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Marisol’s Website",
    "section": "Education",
    "text": "Education\nThe George Washington University (expected graduation: May 2023)"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Marisol’s Website",
    "section": "Work Experience",
    "text": "Work Experience\nCommunications Assistant - GWU Graduate Business School Admissions Office (Sept. 2022 - Present)\nCommunications Intern - TransUnion (May 2022 - Aug. 2022)"
  },
  {
    "objectID": "index.html#student-organizations",
    "href": "index.html#student-organizations",
    "title": "Marisol’s Website",
    "section": "Student Organizations",
    "text": "Student Organizations\nExecutive Producer/Creative Director - GW-TV, GWU’s student-run television station (Aug. 2020 - Present)"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Marisol’s Website",
    "section": "Skills",
    "text": "Skills\nAP Style, Adobe Premier Pro, Adobe Audition, Adobe Illustrator, Google Suite, Microsoft Suite, Salesforce, Asana, Canva, Wix, Google Analytics, R Studio"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi again! This is a portfolio website meant to showcase my past work and experience :)"
  },
  {
    "objectID": "my_article.html",
    "href": "my_article.html",
    "title": "My Publsihed Written Work",
    "section": "",
    "text": "Here is a past article I wrote that was published on The GW Local, GW’s Lifestyle news blog."
  },
  {
    "objectID": "my_article.html#no-time-to-die",
    "href": "my_article.html#no-time-to-die",
    "title": "My Publsihed Written Work",
    "section": "No Time to Die",
    "text": "No Time to Die\nIn Daniel Craig’s last outing as everyone’s favorite secret agent, James Bond, No Time to Die ends Craig’s run with a bang as some critics call it the best performance of Bond in the franchise’s history. The film picks up where the previous Bond film, Spectre, left off. Agent Bond, no longer in active service, is thrown back into action after a friend from the CIA asks for him to go on one last mission to save a kidnapped scientist. As Bond discovers the work of a mysterious villain, he realizes his last outing will be anything but safe.\nWatch if you: are in the mood for an action-filled thriller that will also pack an emotional punch!\nRelease date: out now!"
  },
  {
    "objectID": "my_article.html#dune",
    "href": "my_article.html#dune",
    "title": "My Publsihed Written Work",
    "section": "Dune",
    "text": "Dune\nThis ambitious adaptation of Frank Herbert’s science fiction novel is a film meant to be seen on the big screen. The intense music, visuals, and performances from a star-studded cast including Timothée Chalamet, Jason Momoa, Oscar Issac, and Zendaya makes Dune not just a movie you see, but an experience. Set far in the future, the story revolves around the young Paul Atreides, played by Chalamet, son of a Duke who is the ruler of Caladan (just one planet in a galaxy ruled by an Emperor). As Paul’s family is given more power, chaos breaks out and Paul quickly learns his destiny is much greater than he could have ever imagined. While you can watch Dune on HBO Max, watch it in the theater for the best possible experience! \nWatch if you: are a fan of Star Wars and Game of Thrones… or just Timothée Chalamet!\nRelease date: out now!"
  },
  {
    "objectID": "my_article.html#the-french-dispatch",
    "href": "my_article.html#the-french-dispatch",
    "title": "My Publsihed Written Work",
    "section": "The French Dispatch",
    "text": "The French Dispatch\nWes Anderson’s comedy-drama anthology, The French Dispatch, features Anderson’s trademark meticulous visual style as well as an overwhelmingly talented cast including Bill Murray, Owen Wilson, Elisabeth Moss, Frances McDormand, and once again Timothée Chalamet. The film is a “love letter to journalists,” telling several short stories based around a fictional American newspaper set in a fictional 20th-century French city. Anderson’s idiosyncratic filmmaking style, coupled with the film’s unique story structure makes The French Dispatch unlike any film currently in theaters. \nWatch if you: like aesthetically pleasing films with eccentric characters… or just Timothée Chalamet!\nRelease date: out now!"
  },
  {
    "objectID": "my_article.html#eternals",
    "href": "my_article.html#eternals",
    "title": "My Publsihed Written Work",
    "section": "Eternals",
    "text": "Eternals\nThe Marvel Cinematic Universe is back in full gear with the release of this jam-packed superhero film, but with new faces both in front of and behind the camera. Coming off the heels of her Best Picture and Best Director wins at the Oscars, Chloé Zhao gives a fresh take on the Marvel film franchise as Angelina Jolie, Richard Madden, Kit Harington, Kumail Nanjiani, Gemma Chan, and too many others to list, star in her new film. Based on the immortal alien race from the Marvel comics, Eternals starts off after the events of Avengers: Endgame. The Eternals, who have stayed hidden on Earth, are forced to come together to protect the world from the Deviants, their oldest and deadliest enemy.\nWatch if you: want that epic Marvel movie feel but with fresh, new characters!\nRelease date: Nov. 5!"
  },
  {
    "objectID": "my_article.html#last-night-in-soho",
    "href": "my_article.html#last-night-in-soho",
    "title": "My Publsihed Written Work",
    "section": "Last Night in Soho",
    "text": "Last Night in Soho\nIn this psychological thriller, director Edgar Wright combines horror and time travel with all the flash and style of the 1960s in London. The film begins in the modern day and tells the story of Ellie, a music and fashion lover who travels to London to study fashion design. Feeling ostracized by her peers, Ellie moves out of her dorms and into an apartment where she begins to have vivid dreams of the ’60s in London and an up-and-coming singer named Sandie, which starts to blend with her everyday life. At first enthralled by these visions, Ellie discovers the darkness of the past and why some things should stay hidden.\nWatch if you: want a stylish and eerie psychological thriller to continue the spooky season!\nRelease date: Out now!"
  },
  {
    "objectID": "my_article.html#spencer",
    "href": "my_article.html#spencer",
    "title": "My Publsihed Written Work",
    "section": "Spencer",
    "text": "Spencer\nKristen Stewart gives the performance of her career in Spencer as the late Diana, Princess of Wales, and has already garnered Oscar buzz for the role. In addition to Stewart’s performance, the film is also praised for its beautiful cinematography and regal production design. The story takes place over just a few days, providing a fictional account of the events that took place over the Christmas holidays at the Queen’s estate, the last Christmas before Princess Diana and Prince Charles famously separated.  \nWatch if you: miss The Crown and want to see a more Diana-centered story!\nRelease date: Nov. 5!"
  },
  {
    "objectID": "my_article.html#house-of-gucci",
    "href": "my_article.html#house-of-gucci",
    "title": "My Publsihed Written Work",
    "section": "House of Gucci",
    "text": "House of Gucci\nLady Gaga and Adam Driver lead this jaw-dropping crime drama inspired by true events about the famous Gucci fashion house. In roles worthy of icon status, Gaga and Driver bring the dramatic flair and fashion worthy of the Gucci name. The film takes place over three decades, depicting the scandal, romance, and murder of the then head of the fashion house Maurisio Gucci (played by Driver) who is killed by his ex-wife Patrizia Reggiani (played by Gaga). \nWatch if you: want to see Lady Gaga speak with an Italian accent and wear stunning outfits!\nRelease date: Nov. 24!"
  },
  {
    "objectID": "my_article.html#encanto",
    "href": "my_article.html#encanto",
    "title": "My Publsihed Written Work",
    "section": "Encanto",
    "text": "Encanto\nWith music from Hamilton’s Lin-Manuel Miranda, Encanto is destined to become the next in Disney’s long line of iconic animated musicals. Set in the magical mountains of Colombia, the Madrigal family all have their own special powers, with the sole exception of ordinary Mirabel. However, after learning that her family’s magic is in danger, Mirabel realizes that she is their only hope to save it. Scheduled to premiere Thanksgiving weekend, Encanto is a perfect holiday film for the whole family.\nWatch if you: want that classic Disney feel-good spirit that will tug at your heartstrings!\nRelease date: Nov. 24!"
  },
  {
    "objectID": "my_paper.html",
    "href": "my_paper.html",
    "title": "My Written Work",
    "section": "",
    "text": "Marisol Cabrera\nSMPA 2173\nProfessor Ward\n5 October 2021\nReal World Assignment\nFor my first “Real World” writing assignment I chose a Bloomberg Law article titled, “Nunes Libel Case Raises Actual-Malice Liability for Retweets” by Kyle Jahner and was published recently on September 20, 2021.\nIn 2019, Californian Representative Devin Nunes filed a libel suit, Nunes v. Lizza, against Hearst Magazine Media Inc and writer Ryan Lizza in which Lizza made accusations about the use of undocumented workers at Nunes’ family’s dairy farm. Lizza retweeted this article after the libel suit was filed and before the Judge’s ruling came. Nunes believed Lizza retweeting a story undergoing a libel suit was grounds for actual malice as Lizza did not include the context of Nunes’ denial and the pending suit. In Nunes’ mind this fulfilled the “reckless disregard for the truth’’ aspect of actual malice, a standard set in New York Times v. Sullivan. On the district level the judge dismissed all of Nunes’ claims. However, in 2021 the U.S. Court of Appeals for the Eighth Circuit gave a ruling that while again stated Nunes failed to prove all elements in the libel suit, the court frowned upon Lizza’s decision to retweet the article after the libel suit was filed. This decision by the court signals a potential rehauling of the single publication rule in the light of a new digital age, specifically in relation to social media, and in this case, retweets.\nIn terms of whether or not Lizza acted with actual malice, I believe there are arguments to both sides. “Actual malice” is defined in New York Times v. Sullivan as publishing something “with knowledge that it was false or with reckless disregard of whether it was false or not.” As Lizza’s accused act of actual malice stems from his retweet — when he was aware of Nunes’ denial and suit and did not acknowledge it— the heart of this issue is whether or not retweeting this story should fulfill the “publishing” element of actual malice set in New York Times v. Sullivan.\nOn one hand, the single publication rule states that plaintiffs in a libel case cannot file multiple libel suits even in the case of republication from the same outlet. Lizza’s retweet was of the same story Nunes originally filed his libel suit for, so on the surface it appears clear that the single publication rule would apply and Nunes would be unable to file a separate claim accusing Lizza of actual malice. However the single publication rule no longer applies when the content of the news story changes to include a new libel. Lizza’s retweet came two years after the publication of the original story and Nunes’ denial and suit could be considered the new content to no longer make the single publication rule valid. Although it is questionable whether or not someone’s denial of unflattering claims made against them is significant enough to qualify as new libel. Nunes’ denial could reasonably be expected by readers of the story.\nIt can also be argued that the single publication rule should be dismissed if the second publication reaches a new audience. In its decision the appellate court which ruled on the Nunes case, gave the example of an evening edition of a morning newspaper qualifying as a new publication since the two editions could reasonably be assumed to reach two different audiences. In Lizza’s case, his retweet of the originally printed Esquire article to the online version, two years after the publication, could theoretically be the new audience the court was talking about to be considered a new publication. However, this case is not as clear as the morning to night edition of a newspaper since Lizza’s linking of this story, even after a lengthy amount of time, was not targeting a brand new audience but reminding the public of a story that has become relevant again.\nOverall, I would decide that Lizza did not act with actual malice and that retweeting a story as Lizza did is not a new publication. In an increasingly digital world, journalists have found ways to use social media to best share their work with the public, and in return more and more of the public rely on social media to aggregate relevant and important news articles to consume. I believe if the court were to side with Nunes in this case and rule that Lizza acted with actual malice, there could be a chilling effect on reporters and their willingness to share their work on Twitter. While this might not seem like a big deal, digital clicks and views weigh greatly on which news stories are considered successful and are covered more often versus which are buried and not promoted. While I agree that there is some gray area in Lizza’s case, I think media outlets should be given the benefit of the doubt in unclear cases due to the high priority placed on the freedom of the press, especially related to political matters.\nReferences\nNew York Times Co. v. Sullivan, 376 U.S. 254, 84 S. Ct. 710 (1964)\nNunes v. Lizza, 486 F. Supp. 3d 1267 (N.D. Iowa 2020)\nRoss, Susan Dente, et al. The Law of Journalism and Mass Communication. Seventh edition ed.,\nCQ Press, an Imprint of Sage Publications, Inc., 2020."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Here are some videos I’ve made for GW-TV, my university’s student-run TV station, and other projects I’ve made as a journalism student at the George Washington University."
  },
  {
    "objectID": "portfolio.html#news-episodes",
    "href": "portfolio.html#news-episodes",
    "title": "Portfolio",
    "section": "News Episodes",
    "text": "News Episodes\nHere are two episodes from GWeek, GW-TV’s campus news program, which covers topical events ranging from on campus to international news.\nGWeek 11/16 - Roles: Executive Producer, Editor, Co-Director\nGWeek 10/14 - Roles: Executive Producer, Editor, Co-Director"
  },
  {
    "objectID": "portfolio.html#promo-videos",
    "href": "portfolio.html#promo-videos",
    "title": "Portfolio",
    "section": "Promo Videos",
    "text": "Promo Videos\nObama PSA Project - Roles: Editor, Producer | created for a video production class at GW, meant to practice video editing skills in service of promoting a message.\nGW Balance Promo - Roles: Co-Editor, Co-Producer, Co-Director | created for a web production class at GW, meant to practice filming and editing skills in service of promoting an organization."
  },
  {
    "objectID": "Analysis_Walkthrough.html",
    "href": "Analysis_Walkthrough.html",
    "title": "Analysis Walkthrough",
    "section": "",
    "text": "Functions\nNow we are going to create a function that will return the sum of 2 numbers. Then we’ll run an example of the function.\n\n\nCode\n#naming this function \"summun\" we want the function to run on two arguments, \"x\" and \"y\" which will be the two numbers we want to add together\nsumnum <- function(x, y) {\n  #the function will perform the simple arithmetic function as coded here\n  result <- x+y\n    #we want to make sure the result is actually displayed in the end so we      will make sure to include a return line\n    return(result)\n  } \n\n#Here is the function in action\nsumnum(2,3)\n\n\n[1] 5\n\n\nNow lets create a function that will return the mean of a list of numbers fed to it.\n\n\nCode\n#the argument for this function will be a list of numbers so we will label the argument as \"mean_vector\"\nmean_function <- function(mean_vector){\n  #this function mean to find the mean so we can use the \"mean\" function to perform this task\n  result <- mean(mean_vector)\n  #In order to spit out a result at the end of the function we need to include this \"result\" line\n  return(result)\n}\n\n#Here is the function in action with a set of numbers\nmean_function(c(1,2,3))\n\n\n[1] 2\n\n\nNext, we are adding on to the above function to have it return the results in the form of the sentence “The mean is ___” where the blank is the mean.\n\n\nCode\n#Now we are adding a print factor\nmean_function_print <- function(mean_vector){\n  #Here we want the result object to include this \"paste0\" function in addition to the \"mean\" function. We can have the text always start off with \"The mean is \" making sure to add a space at the end and have the sentence end with what ever mean value to function calculates.\n  result <- paste0(\"The mean is \", mean(mean_vector))\n  #again make sure to include the return line to display a result\n  return(result)\n}\n\n#here is the funciton in action\nmean_function_print(c(1,2,3))\n\n\n[1] \"The mean is 2\"\n\n\nWe will now switch up the dataset used. To download this “flights” data set we need to make sure to have the “nycflights13” package installed. Lets run all of this.\n\n\nCode\n#load the data to use for the rest of the assignment questions\nflights <- nycflights13::flights\n\n\nHere’s the data:\n\n\nCode\n#using the head function gives us the first 6 rows of the data set to get a feel for the data format\nhead(flights)\n\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n  <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n1  2013     1     1      517         515       2     830     819      11 UA     \n2  2013     1     1      533         529       4     850     830      20 UA     \n3  2013     1     1      542         540       2     923     850      33 AA     \n4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n5  2013     1     1      554         600      -6     812     837     -25 DL     \n6  2013     1     1      554         558      -4     740     728      12 UA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nThis data includes records of individual airline flights departing at New York’s three metro airports, JFK, LaGuardia (LGA) and Newark (EWR).\nNow, we will create a function that filters the data frame to only include records from a single originating airport.\n\n\nCode\n#This function's argument will be the abbreviation of the airport name\nfilter_airport <- function(airport){\n  #using the filter function with the airport abbreviation will return information for only the airport selected\n  result <- filter(flights, origin == airport)\n  #make sure to include the return line to display a result\n  return(result)\n}\n\n#example of function\nfilter_airport(\"EWR\")\n\n\n# A tibble: 120,835 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      554        558      -4     740     728      12 UA     \n 3  2013     1     1      555        600      -5     913     854      19 B6     \n 4  2013     1     1      558        600      -2     923     937     -14 UA     \n 5  2013     1     1      559        600      -1     854     902      -8 UA     \n 6  2013     1     1      601        600       1     844     850      -6 B6     \n 7  2013     1     1      606        610      -4     858     910     -12 AA     \n 8  2013     1     1      607        607       0     858     915     -17 UA     \n 9  2013     1     1      608        600       8     807     735      32 MQ     \n10  2013     1     1      615        615       0     833     842      -9 DL     \n# … with 120,825 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nNext, we will create a function that filters the data frame to only include records from a single originating airport and aggregates the results by airlines to show how many flights each airline has from that airport.\n\n\nCode\n#This function will build off our previous function, again the argument remains the same, the airport abbreviation\nnew_filter_airport <- function(airport){\n  #filter function information remains the same\n  flight_data_set <- filter(flights, origin == airport)\n  #the result will change slights to select only the data we want displayed, the carrier, and the count of this value in the other column\n  result <- flight_data_set %>% \n    group_by(carrier) %>% \n    summarise(count=n())\n  #Include this line so result is actually displayed\n  return(result)\n}\n\n#Example of function in action\nnew_filter_airport(\"LGA\")\n\n\n# A tibble: 13 × 2\n   carrier count\n   <chr>   <int>\n 1 9E       2541\n 2 AA      15459\n 3 B6       6002\n 4 DL      23067\n 5 EV       8826\n 6 F9        685\n 7 FL       3260\n 8 MQ      16928\n 9 OO         26\n10 UA       8044\n11 US      13136\n12 WN       6087\n13 YV        601\n\n\n\n\nCensus Data\nNow we will look at census data. The following sections will require use of the tidycensus package, so make sure that is downloaded. Additionally, we will be downloading the data on populations with bachelor’s and graduate/professional degrees.\n\nCredentials\nFirst make sure your API Key credential is loaded. You only have to do this once so download here: https://api.census.gov/data/key_signup.html\nThen after it is downloaded once, comment the following line so you’re code runs correctly.\n\n\nCode\n#census_api_key(\"d0f1c94dadbeb7b2428ea94375fe77ec92f5411f\", install=TRUE)\n\n\nHere are the census variables we will be using:\n\n\nCode\nmyvars <- c(education_total = \"B06009_001\",\n            education_bachelors = \"B06009_005\",\n            education_gradprofess = \"B06009_006\")\n\n\nNow, lets pull down the state-level data for these variables.\n\n\nCode\n#To do this we will use the \"get_acs\" function that is a part of the tidycensus package. Since we want the state level, we will make sure geography = \"state\" and we will make sure geometry = TRUE to pull down the geospatial/mapping data tied to each state\nget_acs(geography = \"state\",\n        variables = myvars,\n        output = \"wide\",\n        geometry = TRUE)\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nSimple feature collection with 52 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1489 ymin: 17.88328 xmax: 179.7785 ymax: 71.36516\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID        NAME education_totalE education_totalM education_bachelorsE\n1     35  New Mexico          1415298              944               223875\n2     72 Puerto Rico               NA               NA                   NA\n3     06  California         26665143             1135              5764827\n4     01     Alabama          3344006             1279               546674\n5     13     Georgia          6996425             2142              1377311\n6     05    Arkansas          2026722             1113               308648\n7     41      Oregon          2944830              938               627360\n8     28 Mississippi          1983112             1576               274566\n9     08    Colorado          3900754             1046              1015989\n10    49        Utah          1868472              685               429936\n   education_bachelorsM education_gradprofessE education_gradprofessM\n1                  3630                 174413                   3764\n2                    NA                     NA                     NA\n3                 21887                3492046                  22526\n4                  6282                 330370                   5948\n5                 11045                 875351                  10767\n6                  5611                 174047                   3342\n7                  6095                 385684                   4750\n8                  5133                 176633                   3755\n9                  7701                 606864                   6979\n10                 5301                 218134                   3337\n                         geometry\n1  MULTIPOLYGON (((-109.0502 3...\n2  MULTIPOLYGON (((-65.23805 1...\n3  MULTIPOLYGON (((-118.6044 3...\n4  MULTIPOLYGON (((-88.05338 3...\n5  MULTIPOLYGON (((-81.27939 3...\n6  MULTIPOLYGON (((-94.61792 3...\n7  MULTIPOLYGON (((-123.6647 4...\n8  MULTIPOLYGON (((-88.50297 3...\n9  MULTIPOLYGON (((-109.0603 3...\n10 MULTIPOLYGON (((-114.053 37...\n\n\nLet’s clean up the data set, removing the three “margin of error” columns\n\n\nCode\n#to do this we will name and save the data set created with the \"get_acs\" function\nallstates_wide <- get_acs(geography = \"state\",\n        variables = myvars,\n        output = \"wide\",\n        geometry = TRUE)\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nCode\n#now lets create a new data set called \"clean_allstates_wide\" that is cleaned up using the \"select\" function removing, using the \"-\" character, all columns that end with \"m\" as that denotes the margin of error columns\nclean_allstates_wide <- allstates_wide %>%\n  select(-ends_with(\"M\"))\n\n#here is the new clean data set\nclean_allstates_wide\n\n\nSimple feature collection with 52 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1489 ymin: 17.88328 xmax: 179.7785 ymax: 71.36516\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID        NAME education_totalE education_bachelorsE\n1     35  New Mexico          1415298               223875\n2     72 Puerto Rico               NA                   NA\n3     06  California         26665143              5764827\n4     01     Alabama          3344006               546674\n5     13     Georgia          6996425              1377311\n6     05    Arkansas          2026722               308648\n7     41      Oregon          2944830               627360\n8     28 Mississippi          1983112               274566\n9     08    Colorado          3900754              1015989\n10    49        Utah          1868472               429936\n   education_gradprofessE                       geometry\n1                  174413 MULTIPOLYGON (((-109.0502 3...\n2                      NA MULTIPOLYGON (((-65.23805 1...\n3                 3492046 MULTIPOLYGON (((-118.6044 3...\n4                  330370 MULTIPOLYGON (((-88.05338 3...\n5                  875351 MULTIPOLYGON (((-81.27939 3...\n6                  174047 MULTIPOLYGON (((-94.61792 3...\n7                  385684 MULTIPOLYGON (((-123.6647 4...\n8                  176633 MULTIPOLYGON (((-88.50297 3...\n9                  606864 MULTIPOLYGON (((-109.0603 3...\n10                 218134 MULTIPOLYGON (((-114.053 37...\n\n\nNow we will create a new column that contains the percentage of people with a bachelor’s or higher degree for each state.\n\n\nCode\n#Since the Census counts people with a bachelor's only vs. a graduate degree separately, we will need to combine the two categories before making our calculation against the total population column. To do this we will use the mutate function to combine \"education_bachelorsE\" and \"education_gradprofessE\"\nnew_clean_allstates_wide <- clean_allstates_wide %>% \n  mutate(bach_or_higher = education_bachelorsE + education_gradprofessE)\n\n#With that new combined column created we can use the mutate function again to create the percentage column\npctdata_new_clean_allstates_wide <- new_clean_allstates_wide %>% \n  mutate(pct_ed_bach_or_higher = (bach_or_higher / education_totalE) *100)\n\n#here is the data set with out new columns\npctdata_new_clean_allstates_wide\n\n\nSimple feature collection with 52 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1489 ymin: 17.88328 xmax: 179.7785 ymax: 71.36516\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID        NAME education_totalE education_bachelorsE\n1     35  New Mexico          1415298               223875\n2     72 Puerto Rico               NA                   NA\n3     06  California         26665143              5764827\n4     01     Alabama          3344006               546674\n5     13     Georgia          6996425              1377311\n6     05    Arkansas          2026722               308648\n7     41      Oregon          2944830               627360\n8     28 Mississippi          1983112               274566\n9     08    Colorado          3900754              1015989\n10    49        Utah          1868472               429936\n   education_gradprofessE                       geometry bach_or_higher\n1                  174413 MULTIPOLYGON (((-109.0502 3...         398288\n2                      NA MULTIPOLYGON (((-65.23805 1...             NA\n3                 3492046 MULTIPOLYGON (((-118.6044 3...        9256873\n4                  330370 MULTIPOLYGON (((-88.05338 3...         877044\n5                  875351 MULTIPOLYGON (((-81.27939 3...        2252662\n6                  174047 MULTIPOLYGON (((-94.61792 3...         482695\n7                  385684 MULTIPOLYGON (((-123.6647 4...        1013044\n8                  176633 MULTIPOLYGON (((-88.50297 3...         451199\n9                  606864 MULTIPOLYGON (((-109.0603 3...        1622853\n10                 218134 MULTIPOLYGON (((-114.053 37...         648070\n   pct_ed_bach_or_higher\n1               28.14164\n2                     NA\n3               34.71526\n4               26.22735\n5               32.19733\n6               23.81654\n7               34.40076\n8               22.75207\n9               41.60357\n10              34.68449\n\n\nNow let’s create a map of the continental U.S. showing the new percentage measure we’ve created.\n\n\nCode\n#First we need to use the filter function to filter out Alaska, Hawaii and Puerto Rico\nNEW_pctdata_new_clean_allstates_wide <- pctdata_new_clean_allstates_wide %>% \n  #using \"!=\" will filter out the values listed\n  filter(NAME != \"Alaska\", NAME !=\"Hawaii\", NAME !=\"Puerto Rico\")\n\n#here is the filtered data\nNEW_pctdata_new_clean_allstates_wide\n\n\nSimple feature collection with 49 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7631 ymin: 24.5231 xmax: -66.9499 ymax: 49.38436\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID        NAME education_totalE education_bachelorsE\n1     35  New Mexico          1415298               223875\n2     06  California         26665143              5764827\n3     01     Alabama          3344006               546674\n4     13     Georgia          6996425              1377311\n5     05    Arkansas          2026722               308648\n6     41      Oregon          2944830               627360\n7     28 Mississippi          1983112               274566\n8     08    Colorado          3900754              1015989\n9     49        Utah          1868472               429936\n10    40    Oklahoma          2611680               448366\n   education_gradprofessE                       geometry bach_or_higher\n1                  174413 MULTIPOLYGON (((-109.0502 3...         398288\n2                 3492046 MULTIPOLYGON (((-118.6044 3...        9256873\n3                  330370 MULTIPOLYGON (((-88.05338 3...         877044\n4                  875351 MULTIPOLYGON (((-81.27939 3...        2252662\n5                  174047 MULTIPOLYGON (((-94.61792 3...         482695\n6                  385684 MULTIPOLYGON (((-123.6647 4...        1013044\n7                  176633 MULTIPOLYGON (((-88.50297 3...         451199\n8                  606864 MULTIPOLYGON (((-109.0603 3...        1622853\n9                  218134 MULTIPOLYGON (((-114.053 37...         648070\n10                 234536 MULTIPOLYGON (((-103.0026 3...         682902\n   pct_ed_bach_or_higher\n1               28.14164\n2               34.71526\n3               26.22735\n4               32.19733\n5               23.81654\n6               34.40076\n7               22.75207\n8               41.60357\n9               34.68449\n10              26.14800\n\n\nCode\n#Now we will use the tmap package to create a shaded map that shows the percentage of Bachelor's-and-higher populations in each state.\n\n#mode = \"plot\" creates a static image for us to analyze\ntmap_mode(mode = \"plot\")\n\n\ntmap mode set to plotting\n\n\nCode\n#make sure to use the new filtered data set for the map\ntm_shape(NEW_pctdata_new_clean_allstates_wide) +\n  #use \"pct_ed_bach_or_higher\" so the package knows which value to shade by and \"GEOID\" so the map is pulling from the right state information\n  tm_polygons(\"pct_ed_bach_or_higher\", id = \"GEOID\")\n\n\n\n\n\n– END —"
  },
  {
    "objectID": "va_elex_project/02_virginia_election_project_youranalysis.html",
    "href": "va_elex_project/02_virginia_election_project_youranalysis.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "The following page will showcase data pertaining to the Virginia results of the 2020 Presidential Election and the 2021 Virginia Gubernatorial race. Comparing the results of these two races can show the changes, if any, of the Democratic and Republican parties within these Virginian localities from 2020 to 2021.\nWe’ll combine two separate data sets from both elections into one set and use the DT package and GGPlot to analyze the data, paying particularly close attention to the percentage of the vote data for each candidate.\nFirst, download necessary packages\nDownload necessary data\n\njoined_vacomparison <- readRDS(here(\"processed_data\", \"joined_vacomparison.rds\"))\n\n\n\nHere is an interactive table of the full data set\n\n#Using the DT package, I am creating an table pulling from the \"joined_vacomparison\" data set we loaded\nDT::datatable(joined_vacomparison, \n              #Since we want a paginated datatable, we are going to make sure to have                        \"rownames\" equal \"TRUE\"\n              rownames = TRUE, \n              #To make the table more interactive we are going to add a \"filer\" option at the                \"top\" of the table to search through data as needed \n              filter = \"top\", \n              #Additionally, we are going to add the option of downloading the data set with                 the \"copy\", \"CSV\", and \"Excel\" buttons using the next 4 lines of code\n              extensions = 'Buttons', \n              options = list(\n                dom = 'Bfrtip',\n                buttons = c('copy', 'csv', \"excel\")\n              )) %>%\n  #To make the locality names clearer, the defining data point of each record, we are going to   use \"formatStyle\" to bold the locality name text and color it red\n  DT::formatStyle('locality',  color = 'red', fontWeight = 'bold')\n\n\n\n\n\n\n\n\n\nHere is a chart showing the Top 5 counties with the highest differences between Younkin/Trump percent\n\n#First, to make this chart we have to create a new column. To do this we are going to use the mutate function. We are using the \"pct_youngkin\" and the \"trump_pct\" columns which both hold the pecentage of the vote each candidate recieved in their race. Subtracting the two will give us the difference which will be displayed in this new \"Y_vs_T\" column in the new Y_T_chart data set which will be used in the following code for the ggplot chart.\nY_T_chart <- joined_vacomparison %>% \n  mutate(\n    Y_vs_T = pct_youngkin - trump_pct\n  )\n\n#In order to get the Top 5 localities like we want, we have to arrange the data from biggest to smallest using the arrange function. We use \"desc\" as well since the default is smallest to largest. \nordered_Y_T_chart <- Y_T_chart %>% arrange(desc(Y_vs_T))\n\n#Now that the data is ordered correctly we can use the head function to get just the first 5 which will be the largest. \nshort_Y_T_chart<- head(ordered_Y_T_chart, 5)\n\n#using GGPlot we can now use this shortened, ordered data as the data set to pull from. Using locality name as the independent variable and our new Youngkin vs Trump column as our dependent variable, this table visualizes the top 5 localties with the largest percentage difference. \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order as well. \n#Using \"fill\" I also assigned the chart a red hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_Y_T_chart, aes(x = reorder(locality, Y_vs_T), y = Y_vs_T)) +\n  geom_col(fill = \"#B33F40\") + \n  scale_y_continuous(name = \"Youngkin percentage points vs Trump\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Youngkin vs Trump Percentage Difference by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n\nHere is a chart that shows the Top 5 counties based on where Youngkin got the highest percentage of the vote\n\n#Since the data for this chart already exists, we can skip the mutate function and jump right to ordering the data set from pct_youngkin largest to smallest\nordered_Y_chart <- Y_T_chart %>% arrange(desc(pct_youngkin))\n\n#Now that it is ordered we can slice off the top 5 rows\nshort_Y_chart<- head(ordered_Y_chart, 5)\n\n#Using this new ordered, shortened data set we can build a GGPlot chart using locality name as the independent variable and Youngkin percentage dependent variable. This table visualizes the top 5 Youngkin percentage share localties.  \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order \n#Using \"fill\" I also assigned the chart a red hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_Y_chart, aes(x = reorder(locality, pct_youngkin), y = pct_youngkin)) +\n  geom_col(fill = \"#B33F40\") + \n  scale_y_continuous(name = \"Youngkin percentage of the vote\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top Youngkin Percentage of the Vote by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n\nHere is a table that shows the Top 5 counties based on where Mcauliffe got the highest percentage of the vote\n\n#Again, since the data for this chart already exists, we can jump right to ordering the data set from pct_mcauliffe largest to smallest\nordered_M_chart <- Y_T_chart %>% arrange(desc(pct_mcauliffe))\n\n#With this properly ordered data we can slice off the top 5 rows\nshort_M_chart <- head(ordered_M_chart, 5)\n\n#using the new data set \"short_M_chart\" we can use the select function to pull out only the relevent data points we want, in this case the locality name and the percent of Mcauliffe's vote in the DT created table.\nshort_M_chart %>% \n  select(locality, pct_mcauliffe) %>% \n  DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))"
  },
  {
    "objectID": "va_elex_project/02_virginia_election_project_youranalysis.html#my-own-analysis",
    "href": "va_elex_project/02_virginia_election_project_youranalysis.html#my-own-analysis",
    "title": "Virginia Election Project",
    "section": "My own analysis",
    "text": "My own analysis\n\n1: Biden vs Mcauliffe\nHere is a chart showing the top 5 localities with the biggest differences between the percentage of the vote from Biden to Mcauliffe.\n\n#First, we need to use the mutate function to create a new column showing the difference between Biden's percentage share and Mcauliffe's. We are using \"biden_pct\" and \"pct_youngkin\" columns which both hold the pecentage of the vote each candidate recieved in their race. By subtracting the two, \"B_vs_M\" will give us the difference which will be used in the following code for the ggplot chart.\nB_M_chart <- Y_T_chart %>% \n  mutate(\n    B_vs_M = biden_pct - pct_mcauliffe\n    )\n#To get the Top 5 localities like we want, we have to arrange the data from biggest to smallest using the arrange function and use \"desc\" as well since the default is smallest to largest. \nordered_B_M_chart <- B_M_chart %>% arrange(desc(B_vs_M))\n\n#Now that the data is ordered correctly we can use the head function to get just the first 5 which will be the largest. \nshort_B_M_chart<- head(ordered_B_M_chart, 5)\n\n\n#Using GGPlot we can now use this shortened, ordered data as the data set to pull from. Using locality name as the independent variable and the new Biden vs Mcauliffe column as our dependent variable, this table visualizes the top 5 localties with the largest percentage difference. \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order as well. \n#Using \"fill\" I also assigned the chart a blue hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_B_M_chart, aes(x = reorder(locality, B_vs_M), y = B_vs_M)) +\n  geom_col(fill = \"#6577B3\") + \n  scale_y_continuous(name = \"Biden percentage points vs Mcauliffe\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Biden vs Mcauliffe Percentage Difference by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n2: Closet localities in gubernatorial race\nHere is a chart showing the top 5 localities that had the closest races in the 2021 gubernatorial race\n\n#Now instead of finding the largest difference we are going to find the closest races. This could be helpful for finding swing districts. \n\n#To do this we need to create a new column finding the difference between Mcaullife and Youngkin's percentage of the vote. We use the absoulte value of the difference since we do not care about who won but by how much, looking at the values closest to zero to find the closest races. The mutate function creates a new column, \"M_vs_Y\" showing this data.\nG_chart <- Y_T_chart %>% \n  mutate(\n    M_vs_Y = abs(pct_mcauliffe - pct_youngkin)\n    )\n\n#Now we order the data with the arrange function from smallest to largest since that will give us the races with the smallest difference or the closest races.\nordered_G_chart <- G_chart %>% arrange(M_vs_Y)\n\n#With this ordered data we will use the head function to select the top 5 closest races. \nshort_ordered_G_chart<- head(ordered_G_chart, 5)\n\n#Using GGPlot, we will pull from the ordered, shortened data set using locality on the X axis, ordered by percentage value in descending order using \"reorder\", and the percentage difference on the Y axis\n#I used a purple hex code to make the chart more visually appealing\n#Additionally, I gave the X and Y axis an appropraite name and the chart a title to make it easier to understand\n#I also used \"coord_flip\" so the X axis had more room to display the locality labels \nggplot(short_ordered_G_chart, aes(x = reorder(locality, M_vs_Y), y = M_vs_Y)) +\n  geom_col(fill = \"#5F396A\") + \n  scale_y_continuous(name = \"Difference in Percentage Points\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top 5 Closest Localities in Gubernatorial Race\") +\n  coord_flip()\n\n\n\n\n\n\n3: Closet localities in presidential race\nHere is a chart showing the top 5 localities that had the closest races in the 2020 presidential race\n\n#Similarly we are going to find the closest races in the presidential race. \n\n#To do this we need to create a new column finding the difference between Biden and Trump's percentage of the vote. Again, using the absoulte value of the difference since we do not care about who won but by how much, looking at the values closest to zero to find the closest races. The mutate function creates a new column, \"B_vs_T\" showing this data.\nP_chart <- Y_T_chart %>% \n  mutate(\n    B_vs_T = abs(biden_pct - trump_pct)\n    )\n\n#Now we will arrange the data with the arrange function from smallest to largest since that will give us the races with the smallest difference or the closest races.\nordered_P_chart <- P_chart %>% arrange(B_vs_T)\n\n#With this ordered data we will use the head function to select the top 5 closest races. \nshort_ordered_P_chart<- head(ordered_P_chart, 5)\n\n#Using GGPlot, we will pull from the ordered, shortened data set using locality on the X axis, ordered by percentage value in descending order using \"reorder\", and the percentage difference on the Y axis\n#To make chart look more visually appealing, we will use a purple hex code in \"fill\"\n#And to make the chart easier to understand, we will give the X and Y axis an appropraite name and the chart a title\n#Also we will used \"coord_flip\" so the X axis has more room to display the locality labels \nggplot(short_ordered_P_chart, aes(x = reorder(locality, B_vs_T), y = B_vs_T)) +\n  geom_col(fill = \"#5F396A\") + \n  scale_y_continuous(name = \"Difference in Percentage Points\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top 5 Closest Localities in Presidential Race\") +\n  coord_flip()"
  },
  {
    "objectID": "va_elex_project/02_virginia_election_project_youranalysis.html#extra-credit",
    "href": "va_elex_project/02_virginia_election_project_youranalysis.html#extra-credit",
    "title": "Virginia Election Project",
    "section": "Extra credit:",
    "text": "Extra credit:\n\n#To turn 4 Table: Mcauliffe top localities into a function we will create a function called \"create_table\"\n#the argument in this case is labeled as \"data_set\" since the name of the data set is what we are going to feed the function to run\ncreate_table <- function(data_set){\n  \n  #first we need to create a properly ordered data set from the pov of mcauliffe's percentage share. Since we want the largest Mcauliffe wins, we will use the arrange function in descending order. \n  ordered_table <- data_set %>% arrange(desc(pct_mcauliffe))\n  \n  #now we want only the top 5 localities, so we will use the head function to select only the top 5, making sure to use the new ordered table so that the top 5 are also the largest. \n  shortened_table <- head(ordered_table, 5)\n  \n  #the final step will be using the DT package to actually create the table. Using the shortened, ordered data set we will use the select function to pull out only the data points we care about and want displayed on the table, the locality name and Mcauliffe's percentage.\n  result <- shortened_table %>% \n    select(locality, pct_mcauliffe) %>% \n    DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))\n  #We want to make sure that the table is actually returned at the end of the function, so we will use \"return\" to call the \"result\" object which is the table we just created\n  return(result)\n}\n\n#here is the function in action creating the same table as in step 4\ncreate_table(Y_T_chart)"
  },
  {
    "objectID": "02_virginia_election_project_youranalysis.html",
    "href": "02_virginia_election_project_youranalysis.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "The following page will showcase data pertaining to the Virginia results of the 2020 Presidential Election and the 2021 Virginia Gubernatorial race. Comparing the results of these two races can show the changes, if any, of the Democratic and Republican parties within these Virginian localities from 2020 to 2021.\nWe’ll combine two separate data sets from both elections into one set and use the DT package and GGPlot to analyze the data, paying particularly close attention to the percentage of the vote data for each candidate.\nFirst, download necessary packages\nDownload necessary data\n\njoined_vacomparison <- readRDS(here(\"processed_data\", \"joined_vacomparison.rds\"))\n\n\n\nHere is an interactive table of the full data set\n\n#Using the DT package, I am creating an table pulling from the \"joined_vacomparison\" data set we loaded\nDT::datatable(joined_vacomparison, \n              #Since we want a paginated datatable, we are going to make sure to have                        \"rownames\" equal \"TRUE\"\n              rownames = TRUE, \n              #To make the table more interactive we are going to add a \"filer\" option at the                \"top\" of the table to search through data as needed \n              filter = \"top\", \n              #Additionally, we are going to add the option of downloading the data set with                 the \"copy\", \"CSV\", and \"Excel\" buttons using the next 4 lines of code\n              extensions = 'Buttons', \n              options = list(\n                dom = 'Bfrtip',\n                buttons = c('copy', 'csv', \"excel\")\n              )) %>%\n  #To make the locality names clearer, the defining data point of each record, we are going to   use \"formatStyle\" to bold the locality name text and color it red\n  DT::formatStyle('locality',  color = 'red', fontWeight = 'bold')\n\n\n\n\n\n\n\n\n\nHere is a chart showing the Top 5 counties with the highest differences between Younkin/Trump percent\n\n#First, to make this chart we have to create a new column. To do this we are going to use the mutate function. We are using the \"pct_youngkin\" and the \"trump_pct\" columns which both hold the pecentage of the vote each candidate recieved in their race. Subtracting the two will give us the difference which will be displayed in this new \"Y_vs_T\" column in the new Y_T_chart data set which will be used in the following code for the ggplot chart.\nY_T_chart <- joined_vacomparison %>% \n  mutate(\n    Y_vs_T = pct_youngkin - trump_pct\n  )\n\n#In order to get the Top 5 localities like we want, we have to arrange the data from biggest to smallest using the arrange function. We use \"desc\" as well since the default is smallest to largest. \nordered_Y_T_chart <- Y_T_chart %>% arrange(desc(Y_vs_T))\n\n#Now that the data is ordered correctly we can use the head function to get just the first 5 which will be the largest. \nshort_Y_T_chart<- head(ordered_Y_T_chart, 5)\n\n#using GGPlot we can now use this shortened, ordered data as the data set to pull from. Using locality name as the independent variable and our new Youngkin vs Trump column as our dependent variable, this table visualizes the top 5 localties with the largest percentage difference. \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order as well. \n#Using \"fill\" I also assigned the chart a red hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_Y_T_chart, aes(x = reorder(locality, Y_vs_T), y = Y_vs_T)) +\n  geom_col(fill = \"#B33F40\") + \n  scale_y_continuous(name = \"Youngkin percentage points vs Trump\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Youngkin vs Trump Percentage Difference by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n\nHere is a chart that shows the Top 5 counties based on where Youngkin got the highest percentage of the vote\n\n#Since the data for this chart already exists, we can skip the mutate function and jump right to ordering the data set from pct_youngkin largest to smallest\nordered_Y_chart <- Y_T_chart %>% arrange(desc(pct_youngkin))\n\n#Now that it is ordered we can slice off the top 5 rows\nshort_Y_chart<- head(ordered_Y_chart, 5)\n\n#Using this new ordered, shortened data set we can build a GGPlot chart using locality name as the independent variable and Youngkin percentage dependent variable. This table visualizes the top 5 Youngkin percentage share localties.  \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order \n#Using \"fill\" I also assigned the chart a red hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_Y_chart, aes(x = reorder(locality, pct_youngkin), y = pct_youngkin)) +\n  geom_col(fill = \"#B33F40\") + \n  scale_y_continuous(name = \"Youngkin percentage of the vote\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top Youngkin Percentage of the Vote by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n\nHere is a table that shows the Top 5 counties based on where Mcauliffe got the highest percentage of the vote\n\n#Again, since the data for this chart already exists, we can jump right to ordering the data set from pct_mcauliffe largest to smallest\nordered_M_chart <- Y_T_chart %>% arrange(desc(pct_mcauliffe))\n\n#With this properly ordered data we can slice off the top 5 rows\nshort_M_chart <- head(ordered_M_chart, 5)\n\n#using the new data set \"short_M_chart\" we can use the select function to pull out only the relevent data points we want, in this case the locality name and the percent of Mcauliffe's vote in the DT created table.\nshort_M_chart %>% \n  select(locality, pct_mcauliffe) %>% \n  DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))"
  },
  {
    "objectID": "02_virginia_election_project_youranalysis.html#my-own-analysis",
    "href": "02_virginia_election_project_youranalysis.html#my-own-analysis",
    "title": "Virginia Election Project",
    "section": "My own analysis",
    "text": "My own analysis\n\n1: Biden vs Mcauliffe\nHere is a chart showing the top 5 localities with the biggest differences between the percentage of the vote from Biden to Mcauliffe.\n\n#First, we need to use the mutate function to create a new column showing the difference between Biden's percentage share and Mcauliffe's. We are using \"biden_pct\" and \"pct_youngkin\" columns which both hold the pecentage of the vote each candidate recieved in their race. By subtracting the two, \"B_vs_M\" will give us the difference which will be used in the following code for the ggplot chart.\nB_M_chart <- Y_T_chart %>% \n  mutate(\n    B_vs_M = biden_pct - pct_mcauliffe\n    )\n#To get the Top 5 localities like we want, we have to arrange the data from biggest to smallest using the arrange function and use \"desc\" as well since the default is smallest to largest. \nordered_B_M_chart <- B_M_chart %>% arrange(desc(B_vs_M))\n\n#Now that the data is ordered correctly we can use the head function to get just the first 5 which will be the largest. \nshort_B_M_chart<- head(ordered_B_M_chart, 5)\n\n\n#Using GGPlot we can now use this shortened, ordered data as the data set to pull from. Using locality name as the independent variable and the new Biden vs Mcauliffe column as our dependent variable, this table visualizes the top 5 localties with the largest percentage difference. \n#we use \"reorder\" for the locality so that the bars appear in descending percentage order as well. \n#Using \"fill\" I also assigned the chart a blue hex code to have a better visual appeal\n#I also assigned a fitting y axis and x axis name as well as a title to make the chart easier to read\n#I used \"coord_flip()\" as the localities names were too mushed together in the default display\nggplot(short_B_M_chart, aes(x = reorder(locality, B_vs_M), y = B_vs_M)) +\n  geom_col(fill = \"#6577B3\") + \n  scale_y_continuous(name = \"Biden percentage points vs Mcauliffe\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Biden vs Mcauliffe Percentage Difference by Locality (Top 5)\") +\n  coord_flip()\n\n\n\n\n\n\n2: Closet localities in gubernatorial race\nHere is a chart showing the top 5 localities that had the closest races in the 2021 gubernatorial race\n\n#Now instead of finding the largest difference we are going to find the closest races. This could be helpful for finding swing districts. \n\n#To do this we need to create a new column finding the difference between Mcaullife and Youngkin's percentage of the vote. We use the absoulte value of the difference since we do not care about who won but by how much, looking at the values closest to zero to find the closest races. The mutate function creates a new column, \"M_vs_Y\" showing this data.\nG_chart <- Y_T_chart %>% \n  mutate(\n    M_vs_Y = abs(pct_mcauliffe - pct_youngkin)\n    )\n\n#Now we order the data with the arrange function from smallest to largest since that will give us the races with the smallest difference or the closest races.\nordered_G_chart <- G_chart %>% arrange(M_vs_Y)\n\n#With this ordered data we will use the head function to select the top 5 closest races. \nshort_ordered_G_chart<- head(ordered_G_chart, 5)\n\n#Using GGPlot, we will pull from the ordered, shortened data set using locality on the X axis, ordered by percentage value in descending order using \"reorder\", and the percentage difference on the Y axis\n#I used a purple hex code to make the chart more visually appealing\n#Additionally, I gave the X and Y axis an appropraite name and the chart a title to make it easier to understand\n#I also used \"coord_flip\" so the X axis had more room to display the locality labels \nggplot(short_ordered_G_chart, aes(x = reorder(locality, M_vs_Y), y = M_vs_Y)) +\n  geom_col(fill = \"#5F396A\") + \n  scale_y_continuous(name = \"Difference in Percentage Points\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top 5 Closest Localities in Gubernatorial Race\") +\n  coord_flip()\n\n\n\n\n\n\n3: Closet localities in presidential race\nHere is a chart showing the top 5 localities that had the closest races in the 2020 presidential race\n\n#Similarly we are going to find the closest races in the presidential race. \n\n#To do this we need to create a new column finding the difference between Biden and Trump's percentage of the vote. Again, using the absoulte value of the difference since we do not care about who won but by how much, looking at the values closest to zero to find the closest races. The mutate function creates a new column, \"B_vs_T\" showing this data.\nP_chart <- Y_T_chart %>% \n  mutate(\n    B_vs_T = abs(biden_pct - trump_pct)\n    )\n\n#Now we will arrange the data with the arrange function from smallest to largest since that will give us the races with the smallest difference or the closest races.\nordered_P_chart <- P_chart %>% arrange(B_vs_T)\n\n#With this ordered data we will use the head function to select the top 5 closest races. \nshort_ordered_P_chart<- head(ordered_P_chart, 5)\n\n#Using GGPlot, we will pull from the ordered, shortened data set using locality on the X axis, ordered by percentage value in descending order using \"reorder\", and the percentage difference on the Y axis\n#To make chart look more visually appealing, we will use a purple hex code in \"fill\"\n#And to make the chart easier to understand, we will give the X and Y axis an appropraite name and the chart a title\n#Also we will used \"coord_flip\" so the X axis has more room to display the locality labels \nggplot(short_ordered_P_chart, aes(x = reorder(locality, B_vs_T), y = B_vs_T)) +\n  geom_col(fill = \"#5F396A\") + \n  scale_y_continuous(name = \"Difference in Percentage Points\") +\n  scale_x_discrete(name = \"Locality Name\") +\n  ggtitle(\"Top 5 Closest Localities in Presidential Race\") +\n  coord_flip()"
  },
  {
    "objectID": "02_virginia_election_project_youranalysis.html#extra-credit",
    "href": "02_virginia_election_project_youranalysis.html#extra-credit",
    "title": "Virginia Election Project",
    "section": "Extra credit:",
    "text": "Extra credit:\n\n#To turn 4 Table: Mcauliffe top localities into a function we will create a function called \"create_table\"\n#the argument in this case is labeled as \"data_set\" since the name of the data set is what we are going to feed the function to run\ncreate_table <- function(data_set){\n  \n  #first we need to create a properly ordered data set from the pov of mcauliffe's percentage share. Since we want the largest Mcauliffe wins, we will use the arrange function in descending order. \n  ordered_table <- data_set %>% arrange(desc(pct_mcauliffe))\n  \n  #now we want only the top 5 localities, so we will use the head function to select only the top 5, making sure to use the new ordered table so that the top 5 are also the largest. \n  shortened_table <- head(ordered_table, 5)\n  \n  #the final step will be using the DT package to actually create the table. Using the shortened, ordered data set we will use the select function to pull out only the data points we care about and want displayed on the table, the locality name and Mcauliffe's percentage.\n  result <- shortened_table %>% \n    select(locality, pct_mcauliffe) %>% \n    DT::datatable(rownames = FALSE, \n                options = list(searching = FALSE, paging = FALSE, dom = \"tip\"))\n  #We want to make sure that the table is actually returned at the end of the function, so we will use \"return\" to call the \"result\" object which is the table we just created\n  return(result)\n}\n\n#here is the function in action creating the same table as in step 4\ncreate_table(Y_T_chart)"
  }
]